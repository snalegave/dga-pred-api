# Data partotion into training and testing set
set.seed(1234)
ind = sample(2,nrow(data), replace=T, prob=c(0.7,0.3))
training = data[ind==1, 1:21]
testing = data[ind==2, 1:21]
training_target = data[ind==1, 22]
testing_target = data[ind==2, 22]
# One Hot Encoding
training_labels = to_categorical(training_target)
testing_labels = to_categorical(testing_target)
View(testing_labels)
View(training_labels)
# Create sequntial model
model = keras_model_sequential()
model %>%
layer_dense(units=8, activation='relu', input_shape=c(21)) %>%
layer_dense(units=3, activation='softmax')
summary(model)
get_layer(model)
get_layer()
model %>% get_layer()
model %>% get_config()
# Compile
# Other options: loss='mean_squared_erorr'
model %>%
compile(loss='categorical_crossentropy',
optimizer='adam',
metrics='accuracy')
# Fit the model
history = model %>%
fit(x=training,
y=training_labels,
epochs=100,
batch_size=32,
validation_split=0.2)
# Evaluate the model with test data
model1 = model %>%
evaluate(testing,testing_labels)
# Prediction and confusion matrix
prob = model %>%
predict_proba(testing)
pred = model %>%
predict_classes(testing)
table1 = table(Predicted=pred, Actual=testing_target)
print(table1)
cbind(prob, pred, testing_target)
# Create sequntial model
model = keras_model_sequential()
model %>%
layer_dense(units=8, activation='relu', input_shape=c(21)) %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=3, activation='softmax')
summary(model)
# Compile
# Other options: loss='mean_squared_erorr'
model %>%
compile(loss='categorical_crossentropy',
optimizer='adam',
metrics='accuracy')
# Fit the model
history = model %>%
fit(x=training,
y=training_labels,
epochs=100,
batch_size=32,
validation_split=0.2)
# Evaluate the model with test data
model2 = model %>%
evaluate(testing,testing_labels)
# Create sequntial model
model = keras_model_sequential()
model %>%
layer_dense(units=8, activation='relu', input_shape=c(21)) %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=3, activation='softmax')
# Compile
# Other options: loss='mean_squared_erorr'
model %>%
compile(loss='categorical_crossentropy',
optimizer='adam',
metrics='accuracy')
# Fit the model
history = model %>%
fit(x=training,
y=training_labels,
epochs=100,
batch_size=32,
validation_split=0.2)
# Evaluate the model with test data
model3 = model %>%
evaluate(testing,testing_labels)
model %>%
layer_dense(units=8, activation='relu', input_shape=c(21)) %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=3, activation='softmax')
# Create sequntial model
model = keras_model_sequential()
model %>%
layer_dense(units=8, activation='relu', input_shape=c(21)) %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=3, activation='softmax')
# Compile
# Other options: loss='mean_squared_erorr'
model %>%
compile(loss='categorical_crossentropy',
optimizer='adam',
metrics='accuracy')
# Fit the model
history = model %>%
fit(x=training,
y=training_labels,
epochs=100,
batch_size=32,
validation_split=0.2)
# Evaluate the model with test data
model3 = model %>%
evaluate(testing,testing_labels)
model %>%
layer_dense(units=8, activation='relu', input_shape=c(21)) %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=3, activation='softmax')
# Create sequntial model
model = keras_model_sequential()
model %>%
layer_dense(units=8, activation='relu', input_shape=c(21)) %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=3, activation='softmax')
# Compile
# Other options: loss='mean_squared_erorr'
model %>%
compile(loss='categorical_crossentropy',
optimizer='adam',
metrics='accuracy')
# Fit the model
history = model %>%
fit(x=training,
y=training_labels,
epochs=100,
batch_size=32,
validation_split=0.2)
# Evaluate the model with test data
model3 = model %>%
evaluate(testing,testing_labels)
# Evaluate the model with test data
model3 = model %>%
evaluate(testing,testing_labels)
# Evaluate the model with test data
model3 = model %>%
evaluate(testing,testing_labels)
# Create sequntial model
model = keras_model_sequential()
model %>%
layer_dense(units=8, activation='relu', input_shape=c(21)) %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=8, activation='relu') %>%
layer_dense(units=3, activation='softmax')
# Compile
# Other options: loss='mean_squared_erorr'
model %>%
compile(loss='categorical_crossentropy',
optimizer='adam',
metrics='accuracy')
# Fit the model
history = model %>%
fit(x=training,
y=training_labels,
epochs=100,
batch_size=32,
validation_split=0.2)
# Evaluate the model with test data
model4 = model %>%
evaluate(testing,testing_labels)
randomForest
library(randomForest)
model_rf = randomForest(training_labels~., data=training)
training_target
model_rf = randomForest(training_target~., data=training)
model
model_rf
model_rf = randomForest(as.factor(training_target)~., data=training)
model_rf
prediction = predict(model_rf, newdata=testing)
table(prediction, testing_target)
model_rf
ted <- read.csv("IMT_574/Data/ted_main.csv", stringsAsFactors = FALSE)
View(ted)
View(ted)
View(ted)
ted <- read.csv("IMT_574/Data/ted_main.csv", stringsAsFactors = FALSE)
row1_tags <- ted$tags[1]
fromJSON(gsub("'", '"', ted$tags[1]))
library(jsonlite)
fromJSON(gsub("'", '"', ted$tags[1]))
row1_tags_lists <- fromJSON(gsub("'", '"', ted$tags[1]))
row1_tags_lists
c("url", "tags")
ted_tags_clean <- ted[,c("url", "tags")]
View(ted_tags_clean)
allratings <- c("a", "b")
all_tags <- c("a", "b")
all_tags <- c('children', 'creativity')
for (i in 1:nrow(ted)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted$tags[i]))
append(all_tags,row_i_tags_list)
}
row1_tags_lists <- fromJSON(gsub("'", '"', ted$tags[1]))
fromJSON(gsub("'", '"', ted$tags[1]))
row1_tags_lists <- fromJSON(gsub("'", '"', ted$tags[100]))
row1_tags_lists
row1_tags_lists <- fromJSON(gsub("'", '"', ted$tags[1480]))
all_tags <- gsub("Alzheimer's","Alzheimer", ted$tags)
View(ted_tags_clean)
all_tags <- gsub("Alzheimer's","Alzheimer", all_tags$tags)
all_tags <- c('children', 'creativity')
ted_tags_clean$tags <- gsub("Alzheimer's","Alzheimer", tags$tags)
ted_tags_clean$tags <- gsub("Alzheimer's","Alzheimer", ted_tags_clean$tags)
View(ted_tags_clean)
for (i in 1:nrow(ted)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
append(all_tags,row_i_tags_list)
}
all_tags <- append(all_tags,row_i_tags_list)
for (i in 1:nrow(ted)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
all_tags <- append(all_tags,row_i_tags_list)
}
all_tags
for (i in 1:nrow(ted)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
all_tags <- append(all_tags,row_i_tags_list)
}
all_tags <- c('children', 'creativity')
all_tags <- NA
for (i in 1:nrow(ted)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
all_tags <- append(all_tags,row_i_tags_list)
}
all_tags
all_tags[-1]
all_tags <- NA
for (i in 1:nrow(ted)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
all_tags <- append(all_tags,row_i_tags_list)
}
all_tags[2:19155]
all_tags <- all_tags[2:19155]
all_tags <- all_tags[-1]
all_tags <- NA
for (i in 1:nrow(ted)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
all_tags <- append(all_tags,row_i_tags_list)
}
all_tags <- all_tags[-1]
unique(all_tags)
unique_tags <- unique(all_tags)
tags_DF <- data.frame(matrix(NA_integer_, nrow = 2550, ncol = 416))
View(tags_DF)
colnames(tags_DF)
colnames(tags_DF) <- unique_tags
tags_DF
View(tags_DF)
for (i in 1:nrow(ted_tags_clean)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
for (tag in row_i_tags_list){
tags_DF[i,tag] <- 1
}
}
View(tags_DF)
View(tags_DF)
tags_DF[1,"delete"] <- 1
View(tags_DF)
tags_DF <- tags_DF[-417]
test <- ted_tags_clean
for (i in 1:nrow(ted_tags_clean)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
for (tag in row_i_tags_list){
test[i,tag] <- 1
}
}
View(test)
for (i in 1:nrow(ted_tags_clean)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
for (tag in row_i_tags_list){
ted_tags_clean[i,tag] <- 1
}
}
ted_tags_clean <- ted_tags_clean[,-c("tags")]
ted_tags_clean <- ted_tags_clean[,-c(2)]
View(ted_tags_clean)
write.csv(ted_tags_clean,"IMT_574/Data/tags_expanded.csv", row.names = FALSE)
View(tags_DF)
View(ted_tags_clean)
View(ted_tags_clean)
# Cleaning up the Tags Column
ted_tags_clean <- ted[,c("url", "tags")]
ted_tags_clean$tags <- gsub("Alzheimer's","Alzheimer", ted_tags_clean$tags)
for (i in 1:nrow(ted_tags_clean)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
for (tag in row_i_tags_list){
ted_tags_clean[i,tag] <- as.factor(1)
}
}
ted_tags_clean[is.na(ted_tags_clean)] = as.factor(0)
View(ted_tags_clean)
warnings()
ted_tags_clean[is.na(ted_tags_clean)]
ted_tags_clean[is.na(ted_tags_clean)] = 0
# Cleaning up the Tags Column
ted_tags_clean <- ted[,c("url", "tags")]
ted_tags_clean$tags <- gsub("Alzheimer's","Alzheimer", ted_tags_clean$tags)
for (i in 1:nrow(ted_tags_clean)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
for (tag in row_i_tags_list){
ted_tags_clean[i,tag] <- 1
}
}
ted_tags_clean[is.na(ted_tags_clean)] = 0
View(ted_tags_clean)
ted_tags_clean <- ted_tags_clean[,-c(2)] # remove the original tags column
class(ted_tags_clean$url)
class(ted_tags_clean$funny)
str(ted_tags_clean)
lapply(ted_tags_clean[2:417] , factor)
ted_tags_clean[2:417] <- lapply(ted_tags_clean[2:417] , factor)
str(ted_tags_clean)
write.csv(ted_tags_clean,"IMT_574/Data/tags_expanded.csv", row.names = FALSE)
View(ted)
ted_ratings <- ted[,c("url","ratings")]
for (i in 1:nrow(ted_ratings)) {
row_ratings <- ted[i, "ratings"]
row_url <- ted[i, "url"]
ratings_json <-  fromJSON(gsub("'", '"', row_ratings))[-1]
total_ratings <- sum(ratings_json$count)
ted_ratings[i,"total_ratings"] <- total_ratings
for (rating_index in 1:nrow(ratings_json)){
rating_name<-ratings_json[rating_index,1]
rating_count <- ratings_json[rating_index,2]/total_ratings
ted_ratings[i,rating_name] <- rating_count
}
}
ted_ratings <- ted_ratings[,-c(2)] # remove original ratings column
# Cleaning up the Tags Column
ted_tags_clean <- ted[,c("url", "tags")]
ted_tags_clean$tags <- gsub("Alzheimer's","Alzheimer", ted_tags_clean$tags)
for (i in 1:nrow(ted_tags_clean)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
for (tag in row_i_tags_list){
ted_tags_clean[i,tag] <- 1
}
}
ted_tags_clean[is.na(ted_tags_clean)] <-  0
ted_tags_clean <- ted_tags_clean[,-c(2)] # remove the original tags column
ted_tags_clean[2:417] <- lapply(ted_tags_clean[2:417] , factor) # convert from numeric to factor
ted_discussions <- ted[,c("url","views", "comments")]
ted_discussions["comment_ratio"] <-  ted_discussions$comments/ted_discussions$views
ted_discussions<- ted_discussions[,c("url", "comment_ratio")]
joined1 <- inner_join(ted_discussions, ted_ratings,"url")
library(tidyr)
library(jsonlite)
library(dplyr)
library(randomForest)
library(neuralnet)
ted <- read.csv("IMT_574/Data/ted_main.csv", stringsAsFactors = FALSE)
ted_ratings <- ted[,c("url","ratings")]
for (i in 1:nrow(ted_ratings)) {
row_ratings <- ted[i, "ratings"]
row_url <- ted[i, "url"]
ratings_json <-  fromJSON(gsub("'", '"', row_ratings))[-1]
total_ratings <- sum(ratings_json$count)
ted_ratings[i,"total_ratings"] <- total_ratings
for (rating_index in 1:nrow(ratings_json)){
rating_name<-ratings_json[rating_index,1]
rating_count <- ratings_json[rating_index,2]/total_ratings
ted_ratings[i,rating_name] <- rating_count
}
}
ted_ratings <- ted_ratings[,-c(2)] # remove original ratings column
write.csv(ted_ratings,"IMT_574/Data/ratings_expanded.csv", row.names = FALSE)
# Cleaning up the Tags Column
ted_tags_clean <- ted[,c("url", "tags")]
ted_tags_clean$tags <- gsub("Alzheimer's","Alzheimer", ted_tags_clean$tags)
for (i in 1:nrow(ted_tags_clean)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
for (tag in row_i_tags_list){
ted_tags_clean[i,tag] <- 1
}
}
ted_tags_clean[is.na(ted_tags_clean)] <-  0
ted_tags_clean <- ted_tags_clean[,-c(2)] # remove the original tags column
ted_tags_clean[2:417] <- lapply(ted_tags_clean[2:417] , factor) # convert from numeric to factor
write.csv(ted_tags_clean,"IMT_574/Data/tags_expanded.csv", row.names = FALSE)
tags_DF[is.na(tags_DF)] <-  0
sum((tags_DF$children))
ted_discussions <- ted[,c("url","views", "comments")]
ted_discussions["comment_ratio"] <-  ted_discussions$comments/ted_discussions$views
ted_discussions<- ted_discussions[,c("url", "comment_ratio")]
joined1 <- inner_join(ted_discussions, ted_ratings,"url")
View(joined1)
joined2 <- inner_join(joined1, ted_tags_clean,"url")
library(tidyr)
library(jsonlite)
library(dplyr)
library(randomForest)
joined2 <- inner_join(joined1, ted_tags_clean,"url")
joined2 <- inner_join(joined1, ted_tags_clean,"url")
joined1 <- joined1 %>% rename(Jaw_dropping=`Jaw-dropping`)
joined1<- joined1[-1]
max = apply(joined1, 2, max)
min = apply(joined1, 2, min)
scaled = as.data.frame(scale(joined1, center=min, scale=max-min))
joined2 <- inner_join(joined1, ted_tags_clean,"url")
joined1 <- inner_join(ted_discussions, ted_ratings,"url")
joined2 <- inner_join(joined1, ted_tags_clean,"url")
joined1 <- joined1 %>% rename(Jaw_dropping=`Jaw-dropping`)
max = apply(joined1, 2, max)
min = apply(joined1, 2, min)
scaled = as.data.frame(scale(joined1, center=min, scale=max-min))
View(joined1)
View(scaled)
joined1 <- inner_join(ted_discussions, ted_ratings,"url")
joined2 <- inner_join(joined1, ted_tags_clean,"url")
joined1 <- joined1 %>% rename(Jaw_dropping=`Jaw-dropping`)
joined1<- joined1[-1]
max = apply(joined1, 2, max)
min = apply(joined1, 2, min)
ted_ratings <- ted[,c("url","ratings")]
# Cleaning up the Tags Column
ted_tags_clean <- ted[,c("url", "tags")]
ted_tags_clean$tags <- gsub("Alzheimer's","Alzheimer", ted_tags_clean$tags)
for (i in 1:nrow(ted_tags_clean)) {
row_i_tags_list <- fromJSON(gsub("'", '"', ted_tags_clean$tags[i]))
for (tag in row_i_tags_list){
ted_tags_clean[i,tag] <- 1
}
}
ted_tags_clean[is.na(ted_tags_clean)] <-  0
ted_tags_clean <- ted_tags_clean[,-c(2)] # remove the original tags column
ted_discussions <- ted[,c("url","views", "comments")]
ted_discussions["comment_ratio"] <-  ted_discussions$comments/ted_discussions$views
joined2 <- inner_join(ted_discussions, ted_tags_clean,"url")
View(joined2)
##
joined2<- joined2[-1]
View(joined2)
ted_discussions <- ted[,c("url","views", "comments")]
ted_discussions["comment_ratio"] <-  ted_discussions$comments/ted_discussions$views
ted_discussions<- ted_discussions[,c("url", "comment_ratio")]
joined2 <- inner_join(ted_discussions, ted_tags_clean,"url")
##
joined2<- joined2[-1]
tmp_m <- lm(comment_rati0 ~., joined2)
tmp_m <- lm(comment_ratio ~., joined2)
summary(tmp_m)
scale(joined2$comment_ratio, center=min(joined2$comment_ratio), scale=max(joined2$comment_ratio)-min(joined2$comment_ratio))
joined2$comment_ratio <- scale(joined2$comment_ratio, center=min(joined2$comment_ratio), scale=max(joined2$comment_ratio)-min(joined2$comment_ratio))
View(joined2)
tmp_m_2 <- lm(comment_ratio ~., joined2)
tmp_m_2
summary(tmp_m_2)
summary(tmp_m)
AIC(tmp_m)
stepAIC(tmp_m)
library(MASS)
stepAIC(tmp_m)
tmp_m_step <-stepAIC(tmp_m)
summary(tmp_m_step)
joined1 <- inner_join(ted_discussions, ted_ratings,"url")
joined1 <- joined1 %>% rename(Jaw_dropping=`Jaw-dropping`)
View(joined1)
joined1 <- inner_join(ted_discussions, ted_ratings,"url")
for (i in 1:nrow(ted_ratings)) {
row_ratings <- ted[i, "ratings"]
row_url <- ted[i, "url"]
ratings_json <-  fromJSON(gsub("'", '"', row_ratings))[-1]
total_ratings <- sum(ratings_json$count)
ted_ratings[i,"total_ratings"] <- total_ratings
for (rating_index in 1:nrow(ratings_json)){
rating_name<-ratings_json[rating_index,1]
rating_count <- ratings_json[rating_index,2]/total_ratings
ted_ratings[i,rating_name] <- rating_count
}
}
ted_ratings <- ted_ratings[,-c(2)] # remove original ratings column
joined1 <- inner_join(ted_discussions, ted_ratings,"url")
joined1 <- joined1 %>% rename(Jaw_dropping=`Jaw-dropping`)
joined1<- joined1[-1]
max = apply(joined1, 2, max)
min = apply(joined1, 2, min)
scaled = as.data.frame(scale(joined1, center=min, scale=max-min))
scaled_tanh =
index = sample(1:nrow(joined1),round(0.75*nrow(joined1)))
training = scaled[index,]
testing = scaled[-index,]
lm_m <- lm(comment_ratio~., data=training)
summary(lm_m)
tmp <- stepAIC(lm_m)
tmp <- stepAIC(lm_m, direction = "backward")
tmp
summary(tmp)
lm_m <- lm(log(comment_ratio)~., data=training)
summary(lm_m)
setwd("C:/Users/snalegave/Dropbox/UW/grad_school/IMT575/dga-pred-api")
cur_data <- read.csv(file = "data/data.csv")
View(cur_data)
