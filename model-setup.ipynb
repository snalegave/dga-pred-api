{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tldextract in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tldextract) (39.1.0)\n",
      "Requirement already satisfied: requests>=2.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tldextract) (2.20.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tldextract) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.1.0->tldextract) (1.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.1.0->tldextract) (2019.11.28)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests-file>=1.4->tldextract) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tldextract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket = 'dga-storage'\n",
    "prefix = 'sagemaker/xgboost_dga'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'data.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "data = pd.read_csv(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_CHARS = 'abcdefghijklmnopqrstuvwxyz0123456789-_\\n'\n",
    "LOOKUP_TABLE = None\n",
    "\n",
    "def encode_fqdn(fqdn='www.google.com'):\n",
    "    global VALID_CHARS\n",
    "    global LOOKUP_TABLE\n",
    "    if not LOOKUP_TABLE:\n",
    "        LOOKUP_TABLE = dict()\n",
    "        idx = 1\n",
    "        for c in VALID_CHARS:\n",
    "            LOOKUP_TABLE[c] = int(idx)\n",
    "            idx += int(1)\n",
    "            \n",
    "    ds = tldextract.extract(fqdn)\n",
    "    domain = ds.domain\n",
    "    rvalue = []\n",
    "    for c in domain:\n",
    "        rvalue.append(str(LOOKUP_TABLE[c]))\n",
    "    for _ in range(len(rvalue), 63):\n",
    "        rvalue.append('0')\n",
    "    return (rvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['domain_type'] = pd.factorize(data['domain_type'])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = []\n",
    "for domain in data['domain']:\n",
    "    domain_features = encode_fqdn(domain)\n",
    "    data_features.append(domain_features)\n",
    "\n",
    "data_features_df = pd.DataFrame(data_features)\n",
    "labels = data['domain_type']\n",
    "full_dataset = pd.concat([labels, data_features_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(full_dataset.sample(frac=1, random_state=1729), [int(0.7 * len(full_dataset)), int(0.9 * len(full_dataset))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "s3_input_validation = boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'}\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(eta=0.3,\n",
    "                        objective='multi:softmax',\n",
    "                        num_class=2,\n",
    "                        num_round=1000)\n",
    "\n",
    "# xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 's3://{}/{}/{}'.format(bucket, prefix, 'train')\n",
    "validation_data = 's3://{}/{}/{}'.format(bucket, prefix, 'validation')\n",
    "train_channel = sagemaker.session.s3_input(train_data, content_type ='csv')\n",
    "valid_channel = sagemaker.session.s3_input(validation_data, content_type ='csv')\n",
    "\n",
    "xgb.fit({'train': train_channel, 'validation': valid_channel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None\n",
    "\n",
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = pd.read_csv('assignment.csv')\n",
    "new_test['threat'] = pd.factorize(new_test['threat'])[0]\n",
    "\n",
    "data_features = []\n",
    "for domain in new_test['domain']:\n",
    "    domain_features = encode_fqdn(domain)\n",
    "    data_features.append(domain_features)\n",
    "\n",
    "data_features_df = pd.DataFrame(data_features)\n",
    "labels = new_test['threat']\n",
    "final_test = pd.concat([labels, data_features_df], axis=1)\n",
    "predictions = predict(final_test.as_matrix()[:, 1:])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calc_specificity(y_true, y_pred, thresh):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_true == 0)) /sum(y_true ==0)\n",
    "\n",
    "thresh = 0.5\n",
    "y_pred = predictions\n",
    "y_pred_binary = np.where(predictions > thresh, 1, 0)\n",
    "y_true = final_test['threat']\n",
    "\n",
    "c_mat = confusion_matrix(y_true, y_pred_binary) ## Predicted vs. actual outcome\n",
    "auc = round(roc_auc_score(y_true, y_pred),4)\n",
    "accuracy = round(accuracy_score(y_true,(y_pred > thresh) ) ,4)\n",
    "recall = round(recall_score(y_true, (y_pred > thresh)),4)\n",
    "precision = round(precision_score(y_true, (y_pred > thresh)),4)\n",
    "specificity = round(calc_specificity(y_true, y_pred, thresh),4)\n",
    "\n",
    "print(f'AUC is: {auc}')\n",
    "print(f'Accuracy is: {accuracy}')\n",
    "print(f'Recall is: {recall}')\n",
    "print(f'Precision is: {precision}')\n",
    "print(f'Specificity is: {specificity}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
